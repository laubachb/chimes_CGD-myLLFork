{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy.spatial.distance import cdist\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58,)\n",
      "0     25.0\n",
      "1     25.0\n",
      "2     25.0\n",
      "3     25.0\n",
      "4     25.0\n",
      "5     25.0\n",
      "6     25.0\n",
      "7     25.0\n",
      "8     25.0\n",
      "9     25.0\n",
      "10    25.0\n",
      "11    25.0\n",
      "12     5.0\n",
      "13     5.0\n",
      "14    20.0\n",
      "15    20.0\n",
      "16    20.0\n",
      "17     2.0\n",
      "18    20.0\n",
      "19    20.0\n",
      "20    14.0\n",
      "21    20.0\n",
      "22     4.0\n",
      "23    20.0\n",
      "24     2.0\n",
      "25    20.0\n",
      "26    20.0\n",
      "27    20.0\n",
      "28    20.0\n",
      "29    20.0\n",
      "30    20.0\n",
      "31    20.0\n",
      "32    20.0\n",
      "33    20.0\n",
      "34    20.0\n",
      "35    20.0\n",
      "36    20.0\n",
      "37    20.0\n",
      "38    20.0\n",
      "39    20.0\n",
      "40    20.0\n",
      "41    20.0\n",
      "42    20.0\n",
      "43    20.0\n",
      "44    20.0\n",
      "45    20.0\n",
      "46    20.0\n",
      "47     4.0\n",
      "48    20.0\n",
      "49    20.0\n",
      "50    20.0\n",
      "51    20.0\n",
      "52    20.0\n",
      "53    20.0\n",
      "54    20.0\n",
      "55    20.0\n",
      "56    20.0\n",
      "57    20.0\n",
      "Name: Total frames, dtype: float64\n",
      "(1116,)\n",
      "1116.0\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Carbon_data_defs.csv', header=1)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "tot_frames = df[\"Total frames\"]\n",
    "tot_frames = tot_frames.dropna()\n",
    "print(np.shape(tot_frames))\n",
    "print(tot_frames)\n",
    "\n",
    "# Initialize an empty list to store the result\n",
    "alc_labels = []\n",
    "\n",
    "# Iterate through the array of integers\n",
    "for index, value in enumerate(tot_frames):\n",
    "    # Duplicate the index based on the value of the element\n",
    "    for _ in range(int(value)):\n",
    "        alc_labels.append(index)\n",
    "\n",
    "# Write the object to a pickle file\n",
    "pickle_filename = \"alc_labels\"\n",
    "with open(pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(alc_labels, pickle_file)\n",
    "\n",
    "print(np.shape(alc_labels))\n",
    "print(sum(tot_frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/blaubach/chimes_CGD-myLLFork/dissimilarity_clustering/alc_pd_2bS\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/blaubach/chimes_CGD-myLLFork/dissimilarity_clustering/alc_pd_2bS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m all_avgs_equilibrium \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 59\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(path)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Sort the filtered files based on file names\u001b[39;00m\n\u001b[1;32m     62\u001b[0m sorted_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(files)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/blaubach/chimes_CGD-myLLFork/dissimilarity_clustering/alc_pd_2bS'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_files(path, specific_chunk_index, file_chunks, mode=\"all\"):\n",
    "\n",
    "    # Choose a specific chunk\n",
    "    specific_chunk = file_chunks[specific_chunk_index]\n",
    "    print(f\"Number of Frames in SP: {len(specific_chunk)}\")\n",
    "\n",
    "    # Determine which files to consider based on the mode\n",
    "    if mode == \"equilibrium\":\n",
    "        start_index = len(specific_chunk) // 2\n",
    "        files_to_process = specific_chunk[start_index:]\n",
    "    elif mode == \"all\":\n",
    "        files_to_process = specific_chunk\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Use 'equilibrium' or 'all'.\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Process the chosen files\n",
    "    for file_name in files_to_process:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        #print(file_path)\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            #print(f\"Processing {mode} files - Normalized second column entries of {file_name}:\")\n",
    "            second_column_entries = []\n",
    "            for line in lines[:60]:\n",
    "                columns = line.split()\n",
    "                if len(columns) >= 2:\n",
    "                    second_column_entry = float(columns[1])\n",
    "                    second_column_entries.append(second_column_entry)\n",
    "            all_data.append(second_column_entries)\n",
    "    #print(np.shape(all_data))\n",
    "    return all_data\n",
    "\n",
    "# Example usage:\n",
    "#os.chdir('..')\n",
    "current_directory = os.getcwd()\n",
    "#print(\"Current Directory:\", current_directory)\n",
    "\n",
    "body = 2\n",
    "path_2b = f\"/alc_pd_{body}bS\"  # Replace this with your actual path\n",
    "path = current_directory + path_2b\n",
    "print(path)\n",
    "\n",
    "# Choose a specific chunk (e.g., Chunk_0) and mode ('equilibrium' or 'all')\n",
    "# specific_chunk_index = 0\n",
    "mode = \"equilibrium\"  # Change this to 'all' if needed\n",
    "all_avgs_equilibrium = []\n",
    "all_data = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Sort the filtered files based on file names\n",
    "sorted_files = sorted(files)\n",
    "# Initialize an empty list to store sublists (chunks) of files\n",
    "file_chunks = []\n",
    "\n",
    "# Initialize the starting index for slicing the sorted files list\n",
    "start_index = 0\n",
    "\n",
    "# Iterate over each chunk size\n",
    "for size in tot_frames:\n",
    "    size = int(size)\n",
    "    # Determine the end index for slicing\n",
    "    end_index = start_index + size\n",
    "    \n",
    "    # Slice the sorted files list to create a chunk\n",
    "    chunk = sorted_files[start_index:end_index]\n",
    "    \n",
    "    # Append the chunk to the list of file chunks\n",
    "    file_chunks.append(chunk)\n",
    "    \n",
    "    # Update the start index for the next chunk\n",
    "    start_index = end_index\n",
    "\n",
    "# Print the list of file chunks\n",
    "#print(file_chunks)\n",
    "\n",
    "for specific_chunk_index in range(len(tot_frames)):\n",
    "\n",
    "    data = process_files(path, specific_chunk_index, file_chunks, mode)\n",
    "    all_data.append(data)\n",
    "    #print(np.shape(all_data))\n",
    "    #print(np.shape(data))\n",
    "    avg_data = np.mean(data, axis = 0)\n",
    "    avg_data = avg_data.reshape(-1,1)\n",
    "    print(f\"Completed SP {specific_chunk_index}\")\n",
    "    all_avgs_equilibrium.append(avg_data)\n",
    "    #print(np.shape(all_avgs_equilibrium))\n",
    "\n",
    "# print(\"All data:\" ,np.shape(all_data))\n",
    "# print(\"Avg. data:\", np.shape(all_avgs_equilibrium))\n",
    "\n",
    "# Write the object to a pickle file\n",
    "pickle_filename = f\"{body}b_alc_pd_{mode}.ary\"\n",
    "with open(pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(all_data, pickle_file)\n",
    "\n",
    "# Write the object to a pickle file\n",
    "pickle_filename = f\"{body}b_alc_avg_pd_{mode}.ary\"\n",
    "with open(pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(all_avgs_equilibrium, pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
